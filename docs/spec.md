Это внутренний Telegram-бот для «расшифровки совещаний в один клик». Пользователь отправляет аудио или видео — бот извлекает звук (ffmpeg), локально распознаёт речь, разделяет по спикерам и сопоставляет их с профилями, а затем с помощью Qwen3-30B формирует структурное саммари и прикрепляет полный транскрипт (SRT/MD).
Всё работает офлайн во внутреннем контуре: данные не покидают компанию, результаты кешируются по контентному хэшу — повторные файлы обрабатываются мгновенно.
Ценность: быстрые протоколы встреч, явные решения и action items, поиск по базе разговоров, приватность «by design».
Масштабирование: разработка на Mac локально, прод — на GPU-сервере;

⸻

Архитектура (офлайн, on-prem)

Компоненты
 • Telegram Bot (Python, aiogram): прием файлов, статусы, выдача результата.
 • API-шлюз (FastAPI): единая точка входа, авторизация, троттлинг.
 • Очередь задач: Celery + Redis/RabbitMQ.
 • Видеопайплайн: ffmpeg → нормализация аудио (mono, 16kHz).
 • ASR: локально (см. ниже варианты).
 • Диризация и идентификация спикеров: VAD + кластеризация + сопоставление с голосовыми профилями.
 • Суммаризация: Qwen3-30B через vLLM/LM Studio в dev, vLLM на проде.
 • Хранилище: MinIO (S3-совместимое) для исходников и артефактов; PostgreSQL для метаданных.
 • Кеш/дедуп: хэши файлов и контентный аудио-отпечаток.
 • Мониторинг: Prometheus + Grafana, Sentry/Opentelemetry для трассировок.
 • Управление доступом: Keycloak или внутренний OAuth/OIDC, журнал аудита.

⸻

Поток обработки

 1. Bot принимает аудио/видео → сохраняет в MinIO → кладет задачу в очередь.
 2. Дедуп:
 • sha256 исходного файла.
 • Дополнительно: контентный хэш после декодирования (PCM mono/16k, фиксированный гейн) или акустический отпечаток (Chromaprint). Это покрывает «тот же контент в другом контейнере/битрейте».
 • Если найден готовый результат — сразу отдаем summary и файлы.
 3. ffmpeg вытаскивает аудио, нормализует (RMS/LUFS).
 4. VAD (Silero/pyannote) режет на куски с overlap; длинные митинги — батчим.
 5. ASR → получаем текст + таймкоды.
 6. Диризация:
 • Эмбеддинги голосов (ECAPA-TDNN/x-vector), кластеризация (spectral/ahc).
 • Сопоставление именам: сравнение кластеров с профилями пользователей (предзаписанные эталоны по 30–60 сек). Неуверенные — помечаем Speaker 1/2 и просим в чате закрепить соответствие; сохраняем маппинг для будущего.
 7. Пост-процессинг: пунктуация/нормализация, склейка сегментов по спикерам.
 8. Суммаризация (Qwen3-30B) с промптом ниже → JSON + человекочитаемая версия.
 9. Выдача:
 • Сообщение с кратким саммари.
 • Файлы: transcript.md (по спикерам с таймкодами), transcript.vtt/.srt, summary.json.
 • (Опционально) actions.csv / ICS с дедлайнами.

⸻

Модели и инструменты

ASR (локально)
 • WhisperX (Whisper + выравнивание и PID): точные таймкоды, хорошо держит рус/англ. В dev — whisper.cpp/faster-whisper (CTranslate2, Metal на Mac, CUDA на проде).
 • Альтернатива enterprise-стэком: NVIDIA NeMo ASR (QuartzNet/Conformer) + их diarization pipeline.

Для коммерческого использования у pyannote были лицензионные ограничения — проверьте текущие условия. Если не ок, используйте NeMo diarization или собственный стек (ECAPA-TDNN + кластеризация).

Диризация/идентификация
 • VAD: Silero/torchaudio.
 • Спикер-эмбеддинги: ECAPA-TDNN (speechbrain/nemo).
 • Сопоставление именам: cosine similarity + порог; храните несколько эталонов на человека.

LLM (суммаризация)
 • Dev: LM Studio с Qwen3-X (7–14B, GGUF, Metal).
 • Prod: Qwen3-30B в vLLM (PagedAttention, tensor-parallel).
 • FP16 ~60–65 GB VRAM; 4-бит QLoRA/GPTQ ~15–20 GB (для суммаризации обычно ок).
 • Контекст 16–32k, температурa 0.2–0.4, max_new_tokens 1–2k.

⸻

Telegram-бот и UX
 • Фреймворк: aiogram.
 • Файлы >50–100 МБ — сразу переводите в document, не как «audio».
 • Поскольку распознавание не мгновенное: отправляйте progress-сообщения («извлекаю аудио», «распознаю», «суммирую»).
 • Команды:
 • /enroll — добавить голосовой профиль (загрузить 30–60 сек эталона).
 • /link Speaker 2 = Иван Петров — закрепить соответствие постфактум.
 • /privacy — политика и сроки хранения.
 • /delete <meeting_id> — удаление артефактов.

⸻

Структура БД (упрощенно)
 • files(id, sha256, content_fp, mime, duration, size, owner, created_at)
 • meetings(id, file_id, status, language, started_at, finished_at)
 • speakers(id, meeting_id, cluster_id, user_profile_id, confidence)
 • user_profiles(id, display_name, embeddings[], created_at)
 • artifacts(meeting_id, type, s3_key, meta)  // transcript.md, srt, summary.json
 • cache(key, value, ttl)  // промежуточные шаги

⸻

Промпт для суммаризации (рабочий базис)

System:
«Ты помощник по протоколированию встреч. На входе — хронологический транскрипт с таймкодами и спикерами.
Задача: структурированно и кратко оформить итоги. Пиши на русском, делай выводы, не пересказывай лишнее.»

User (пример):

Дай JSON и краткий текст.
JSON-структура:
{
  "summary": "краткое резюме в 5-8 предложений",
  "decisions": [{"who": "Имя", "what": "Решение", "when": "дата/срок или null"}],
  "action_items": [{"assignee":"Имя","task":"...", "due":"YYYY-MM-DD|null", "priority":"low|med|high"}],
  "risks": ["..."],
  "open_questions": ["..."],
  "timeline": [{"t":"00:12:05","topic":"..."}],
  "participants": [{"name":"Имя","speakers":["S1","S3"]}]
}
Правила:

- Ссылайся на таймкоды важных моментов.
- Объединяй одного человека с несколькими кластерами, если контекст совпадает.
- Если имени нет — оставь S#.
Текстовую выжимку дай после JSON.
<ТРАНСКРИПТ ЗДЕСЬ>

⸻

Производительность и качество
 • Чанкование: 60–120 с, overlap 0.2–0.5 с, склейка по тишине/пунктуации.
 • Нормализация громкости: target −23 LUFS (или просто RMS-нормализация), убирает перепады.
 • Язык: автоопределение на первых N сек; подбирайте модель языка.
 • Стабильность: ретраи шагов, idempotency-ключ = content_fp.
 • Валидация: для длинных митингов генерируйте «промежуточные итоговые» каждые 30–45 мин.

⸻

Безопасность и приватность
 • Шифрование в покое (SSE-S3 в MinIO) и в транзите (mTLS во внутренней сети).
 • Ротация ключей, ограничение доступа по ролям (RBAC).
 • Политика хранения: напр., «сырые файлы — 30 дней, артефакты — 180 дней», конфигурируемо.
 • Полный офлайн: ни один шаг не обращается в Интернет.

⸻

Развертывание
 • Dev (Mac Mini M4 Pro, 64 GB):
 • ffmpeg, whisper.cpp/faster-whisper с Metal.
 • LM Studio с Qwen-7/14B (GGUF) для прототипа.
 • Docker Compose: bot, api, queue, minio, pg, grafana.
 • Prod (GPU, Ubuntu):
 • NVIDIA Container Toolkit, CUDA 12.x.
 • vLLM с Qwen3-30B (tp=2/4 по числу GPU).
 • ASR на CUDA (ctranslate2 GPU или NeMo).
 • Горизонтальное масштабирование воркеров ASR; LLM — отдельно.

⸻

Подводные камни и как обойти
 • pyannote лицензии/веса — заранее проверьте условия; запасной вариант: NeMo diarization.
 • Телеграм лимиты по размеру — большие видео лучше конвертить локально и загружать как документ; можно добавить вход через S3-URL/внутренний веб-загрузчик.
 • Разные контейнеры → один контент — используйте контентный хэш после декодирования/Chromaprint.
 • Шум/перебивания — агрессивный VAD + доп. шумоподавление (RNNoise/ffmpeg -af arnndn) повышают точность.
 • Имена спикеров — без энролмента LLM может угадывать из фраз «Павел: …», но финальный маппинг закрепляйте руками для надежности.

⸻

Быстрый MVP (2–3 недели)

 1. Видео→аудио (ffmpeg) → Whisper (faster-whisper) → базовая диризация (кластеризация) без имен.
 2. Суммаризация Qwen-7/14B (LM Studio) → Markdown-отчет + SRT.
 3. Дедуп по sha256 + контентному хэшу.
 4. /enroll и ручной маппинг имен → v2.
 5. vLLM + Qwen3-30B, улучшенная диризация/идентификация → v3.
