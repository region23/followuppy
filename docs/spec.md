Это внутренний Telegram-бот для «расшифровки совещаний в один клик». Пользователь отправляет аудио или видео — бот извлекает звук (ffmpeg), локально распознаёт речь, разделяет по спикерам и сопоставляет их с профилями, а затем с помощью Qwen3-30B формирует структурное саммари и прикрепляет полный транскрипт (SRT/MD).
Всё работает офлайн во внутреннем контуре: данные не покидают компанию, результаты кешируются по контентному хэшу — повторные файлы обрабатываются мгновенно.
Ценность: быстрые протоколы встреч, явные решения и action items, поиск по базе разговоров, приватность «by design».
Масштабирование: разработка на Mac локально, прод — на GPU-сервере;

## Архитектура (офлайн, on-prem)

**Компоненты**

* **Telegram Bot** (Python, `aiogram`): прием файлов, статусы, выдача результата.
* **API-шлюз** (FastAPI): единая точка входа, авторизация, троттлинг.
* **Очередь задач**: Celery + Redis.
* **Видеопайплайн**: `ffmpeg` → нормализация аудио (mono, 16kHz).
* **ASR**: локально (см. ниже варианты).
* **Диризация и идентификация спикеров**: VAD + кластеризация + сопоставление с голосовыми профилями.
* **Суммаризация**: Qwen3-30B через vLLM/LM Studio в dev, vLLM на проде.
* **Хранилище**: SeaweedFS (S3-совместимое) + boto3 для хранения аудио/фидео файлов; PostgreSQL для метаданных, распознанных транскриптов и саммари.
* **Кеш/дедуп**: хэши файлов и **контентный аудио-отпечаток**.
* **Мониторинг**: Sentry/Opentelemetry для трассировок.
* **Управление доступом**: Telegram passport.

---

## Поток обработки

1. Bot принимает аудио/видео → сохраняет в SeaweedFS → кладет задачу в очередь.
2. **Дедуп**:

   * `sha256` исходного файла.
   * Дополнительно: **контентный хэш** после декодирования (PCM mono/16k, фиксированный гейн) **или** акустический отпечаток (Chromaprint). Это покрывает «тот же контент в другом контейнере/битрейте».
   * Если найден готовый результат — сразу отдаем summary и файлы.
3. `ffmpeg` вытаскивает аудио, нормализует (RMS/LUFS).
4. **VAD** (Silero/pyannote) режет на куски с overlap; длинные митинги — батчим.
5. **ASR** → получаем текст + таймкоды.
6. **Диризация**:

   * Эмбеддинги голосов (ECAPA-TDNN/x-vector), кластеризация (spectral/ahc).
   * **Сопоставление именам**: сравнение кластеров с **профилями пользователей** (предзаписанные эталоны по 30–60 сек). Неуверенные — помечаем `Speaker 1/2` и просим в чате закрепить соответствие; сохраняем маппинг для будущего.
7. Пост-процессинг: пунктуация/нормализация, склейка сегментов по спикерам.
8. **Суммаризация** (Qwen3-30B) с промптом ниже → JSON + человекочитаемая версия.
9. Выдача:

   * Сообщение с кратким саммари.
   * Файлы: `transcript.md` (по спикерам с таймкодами), `transcript.vtt/.srt`, `summary.json`.
   * (Опционально) `actions.csv` / ICS с дедлайнами.

---

## Модели и инструменты

**ASR (локально)**

* **WhisperX** (Whisper + выравнивание и PID): точные таймкоды, хорошо держит рус/англ. В dev — `whisper.cpp`/`faster-whisper` (CTranslate2, Metal на Mac, CUDA на проде).
* Альтернатива enterprise-стэком: **NVIDIA NeMo ASR** (QuartzNet/Conformer) + их diarization pipeline.

> Для коммерческого использования у pyannote были лицензионные ограничения — проверьте текущие условия. Если не ок, используйте NeMo diarization или собственный стек (ECAPA-TDNN + кластеризация).

**Диризация/идентификация**

* VAD: Silero/torchaudio.
* Спикер-эмбеддинги: **ECAPA-TDNN** (speechbrain/nemo).
* Сопоставление именам: cosine similarity + порог; храните несколько эталонов на человека.

**LLM (суммаризация)**

* Температура 0.2–0.4 и max_new_tokens 1–2 k подходят для выжимки важных решений.
* Рекомендация: протестировать несколько вариантов промпта на метриках ROUGE/QA-кейзах и добавить fallback-стратегию (разбивку очень длинных транскриптов по частям).

* Dev: LM Studio с Qwen3-X (7–14B, GGUF, Metal).
* Prod: **Qwen3-30B** в **vLLM** (PagedAttention, tensor-parallel).

  * FP16 \~60–65 GB VRAM; 4-бит QLoRA/GPTQ \~15–20 GB (для суммаризации обычно ок).
  * Контекст 16–32k, температурa 0.2–0.4, `max_new_tokens` 1–2k.

---

## Telegram-бот и UX

* Фреймворк: `aiogram`.
* Файлы >50 МБ — сразу переводите в **document**, не как «audio».
* Поскольку распознавание не мгновенное: отправляйте **progress-сообщения** («извлекаю аудио», «распознаю», «суммирую»).
* Рекомендация: добавить rate limit на уровне FastAPI (например, через `slowapi`) и учитывать Telegram-HTTP-timeouts при больших файлах.
* Команды:

  * `/list` — показывает список всех суммаризаций с пагинацией.
  * `/enroll` — добавить голосовой профиль (загрузить 30–60 сек эталона).
  * `/link Speaker 2 = Иван Петров` — закрепить соответствие постфактум.
  * `/privacy` — политика и сроки хранения.
  * `/stats` — ✨ статистика пользователя
  * `/help` — справка по командам
  * `/delete <meeting_id>` — удаление артефактов.

---

## Структура БД (упрощенно)

* `files(id, sha256, content_fp, mime, duration, size, owner, created_at)`
* `meetings(id, file_id, status, language, started_at, finished_at)`
* `speakers(id, meeting_id, cluster_id, user_profile_id, confidence)`
* `user_profiles(id, display_name, embeddings[], created_at)`
* `artifacts(meeting_id, type, s3_key, meta)`  // transcript.md, srt, summary.json
* `cache(key, value, ttl)`  // промежуточные шаги

---

## Промпт для суммаризации (рабочий базис)

**System**:
«Ты помощник по протоколированию встреч. На входе — хронологический транскрипт с таймкодами и спикерами.
Задача: структурированно и кратко оформить итоги. Пиши на русском, делай выводы, не пересказывай лишнее.»

**User** (пример):

```
Дай JSON и краткий текст.
JSON-структура:
{
  "summary": "краткое резюме в 5-8 предложений",
  "decisions": [{"who": "Имя", "what": "Решение", "when": "дата/срок или null"}],
  "action_items": [{"assignee":"Имя","task":"...", "due":"YYYY-MM-DD|null", "priority":"low|med|high"}],
  "risks": ["..."],
  "open_questions": ["..."],
  "timeline": [{"t":"00:12:05","topic":"..."}],
  "participants": [{"name":"Имя","speakers":["S1","S3"]}]
}
Правила:
- Ссылайся на таймкоды важных моментов.
- Объединяй одного человека с несколькими кластерами, если контекст совпадает.
- Если имени нет — оставь S#.
Текстовую выжимку дай после JSON.
<ТРАНСКРИПТ ЗДЕСЬ>
```

---

## Производительность и качество

* **Чанкование**: 60–120 с, overlap 0.2–0.5 с, склейка по тишине/пунктуации.
* **Нормализация громкости**: target −23 LUFS (или просто RMS-нормализация), убирает перепады.
* **Язык**: автоопределение на первых N сек; подбирайте модель языка.
* **Стабильность**: ретраи шагов, idempotency-ключ = `content_fp`.
* **Валидация**: для длинных митингов генерируйте «промежуточные итоговые» каждые 30–45 мин.

---

## Безопасность и приватность

* Шифрование at-rest (AES256 для файлов в SeaweedFS) и in-transit (TLS для API-шлюза).
* Ограничение доступа по ролям (RBAC).
* Политика хранения: напр., «сырые файлы — 30 дней, артефакты — 180 дней», конфигурируемо.
* Полный офлайн: ни один шаг не обращается в Интернет.

---

## Развертывание

* **Dev (Mac Mini M4 Pro, 64 GB):**

  * `ffmpeg`, `whisper.cpp`/`faster-whisper` с Metal.
  * LM Studio с Qwen-7/14B (GGUF) для прототипа.
  * Docker Compose: bot, api, queue, minio, pg, grafana.
  
* **Prod (GPU, Ubuntu):**

  * NVIDIA Container Toolkit, CUDA 12.x.
  * vLLM с Qwen3-30B (tp=2/4 по числу GPU).
  * ASR на CUDA (ctranslate2 GPU или NeMo).
  * Горизонтальное масштабирование воркеров ASR; LLM — отдельно.

---

## Подводные камни и как обойти

* **pyannote лицензии/веса** — заранее проверьте условия; запасной вариант: NeMo diarization.
* **Телеграм лимиты по размеру** — большие видео лучше конвертить локально и загружать как документ; можно добавить вход через S3-URL/внутренний веб-загрузчик.
* **Разные контейнеры → один контент** — используйте контентный хэш после декодирования/Chromaprint.
* **Шум/перебивания** — агрессивный VAD + доп. шумоподавление (RNNoise/`ffmpeg -af arnndn`) повышают точность.
* **Имена спикеров** — без энролмента LLM может угадывать из фраз «Павел: ...», но финальный маппинг закрепляйте руками для надежности.
